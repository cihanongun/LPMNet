{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /home/cvlab/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/cvlab/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/cvlab/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/cvlab/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/cvlab/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/cvlab/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/cvlab/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/cvlab/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "/home/cvlab/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/cvlab/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/cvlab/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/cvlab/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/cvlab/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/cvlab/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import model\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "log_folder = \"logs2/\" # folder path to save the results\n",
    "save_results = True # save the results to log_folder\n",
    "latent_size = 128 # bottleneck size of the Autoencoder model\n",
    "\n",
    "category = \"Chair\"\n",
    "n_points = 2048\n",
    "\n",
    "if(save_results):\n",
    "    utils.clear_folder(log_folder)\n",
    "    writer = SummaryWriter(log_folder + \"TB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape :(2658, 2048, 4)\n",
      "Validation set shape :(396, 2048, 4)\n",
      "Number of points : 2048\n",
      "Part count : 4\n"
     ]
    }
   ],
   "source": [
    "from data.load_dataset import get_dataset\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_set = get_dataset(category, \"train\", n_points)\n",
    "val_set = get_dataset(category, \"validation\", n_points)\n",
    "\n",
    "part_count = int(train_set.max())\n",
    "\n",
    "print(\"Train set shape :\" + str(train_set.shape))\n",
    "print(\"Validation set shape :\" + str(val_set.shape))\n",
    "print(\"Number of points : \" + str(n_points))\n",
    "print(\"Part count : \" + str(part_count))\n",
    "\n",
    "train_tensor = torch.from_numpy(train_set).float()\n",
    "val_tensor = torch.from_numpy(val_set).float()\n",
    "\n",
    "train_loader = DataLoader(dataset=train_tensor, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(dataset=val_tensor, batch_size=batch_size, shuffle=True,  pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LPMNet(\n",
       "  (enc_conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
       "  (enc_conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "  (enc_conv3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "  (enc_bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (enc_bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (seg_conv1): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "  (seg_conv2): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "  (seg_conv3): Conv1d(64, 32, kernel_size=(1,), stride=(1,))\n",
       "  (seg_conv4): Conv1d(32, 5, kernel_size=(1,), stride=(1,))\n",
       "  (seg_bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (seg_bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (seg_bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dec1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "  (dec2): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "  (dec3): Linear(in_features=2048, out_features=6144, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.LPMNet(n_points, latent_size, part_count)\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.loss import chamfer_distance # chamfer distance for calculating point cloud distance\n",
    "\n",
    "def rec_criterion(pc1, pc2):\n",
    "    loss, _ = chamfer_distance(pc1, pc2)\n",
    "    return loss\n",
    "\n",
    "seg_criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch_n):\n",
    "    model.train()\n",
    "    t_rec_loss, t_seg_loss , t_accuracy = 0,0,0\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        labels = data[:,:,3].to(device).long()\n",
    "        \n",
    "        points = data[:,:,0:3].to(device)\n",
    "        \n",
    "        seg_results, output = model(points)\n",
    "        rec_loss = rec_criterion(points, output)\n",
    "        \n",
    "        seg_loss = seg_criterion( seg_results.view(-1,part_count+1) ,labels.view(-1))\n",
    "\n",
    "        seg_labels = seg_results.argmax(dim=2,keepdim=True).squeeze()\n",
    "        correct = seg_labels.eq(labels.data).cpu().sum()\n",
    "        accuracy = correct.item()/float(data.shape[0]*data.shape[1])\n",
    "        \n",
    "        loss = rec_loss + seg_loss\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        t_rec_loss += rec_loss.item()\n",
    "        t_seg_loss += seg_loss.item()\n",
    "        t_accuracy += accuracy\n",
    "        \n",
    "    model.eval()\n",
    "    return t_rec_loss/(i+1) , t_seg_loss/(i+1), t_accuracy/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(epoch_n):\n",
    "    \n",
    "    t_rec_loss, t_seg_loss , t_accuracy = 0,0,0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, data in enumerate(val_loader):\n",
    "\n",
    "            labels = data[:,:,3].to(device).long()\n",
    "\n",
    "            points = data[:,:,0:3].to(device)\n",
    "\n",
    "            seg_results, output = model(points)\n",
    "\n",
    "            rec_loss = rec_criterion(points, output)\n",
    "\n",
    "            seg_loss = seg_criterion( seg_results.view(-1,part_count+1) ,labels.view(-1))\n",
    "\n",
    "            seg_labels = seg_results.argmax(dim=2,keepdim=True).squeeze()\n",
    "            correct = seg_labels.eq(labels.data).cpu().sum()\n",
    "            accuracy = correct.item()/float(data.shape[0]*data.shape[1])\n",
    "\n",
    "            t_rec_loss += rec_loss.item()\n",
    "            t_seg_loss += seg_loss.item()\n",
    "            t_accuracy += accuracy\n",
    "        \n",
    "    return t_rec_loss/(i+1) , t_seg_loss/(i+1), t_accuracy/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(data): # test with a batch of inputs\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        labels = data[:,:,3].to(device).long()\n",
    "        points = data[:,:,0:3].to(device)\n",
    "        \n",
    "        seg_results, output = model(points)\n",
    "        rec_loss = rec_criterion(points, output)\n",
    "        \n",
    "        seg_loss = seg_criterion( seg_results.view(-1,part_count+1) ,labels.view(-1))\n",
    "\n",
    "        seg_labels = seg_results.argmax(dim=2,keepdim=True).squeeze()\n",
    "        correct = seg_labels.eq(labels.data).cpu().sum()\n",
    "        accuracy = correct.item()/float(data.shape[0]*data.shape[1])\n",
    "        \n",
    "        loss = seg_loss + rec_loss\n",
    "        \n",
    "    return accuracy, rec_loss.item(), seg_loss.item(), output.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentall(pc):\n",
    "    \n",
    "    t_data = torch.cat([pc, torch.zeros([pc.shape[0],n_points,1]).to(pc.device)],2)\n",
    "\n",
    "    seg_results, output = model(t_data.to(device))\n",
    "        \n",
    "    seg_labels = seg_results.argmax(dim=2,keepdim=True).squeeze()\n",
    "        \n",
    "    t_data[:,:,3] = seg_labels\n",
    "    \n",
    "    return t_data.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(101) :\n",
    "\n",
    "    startTime = time.time()\n",
    "    \n",
    "    train_rec_loss, train_seg_loss, train_acc = train_epoch(i)\n",
    " \n",
    "    test_rec_loss, test_seg_loss, test_acc = test_epoch(i) # test with test set\n",
    "    \n",
    "    epoch_time = time.time() - startTime\n",
    "    \n",
    "    writeString = \"epoch \" + str(i) + \" epoch time : \" + str(epoch_time) + \"\\n\" + \\\n",
    "                  \"Train --> Rec : \" + str(train_rec_loss) + \" Seg : \" + str(train_seg_loss) + \" Acc : \" + str(train_acc) + \"\\n\" + \\\n",
    "                  \"Validation --> Rec : \" + str(test_rec_loss) + \" Seg : \" + str(test_seg_loss) + \" Acc : \" + str(test_acc) + \"\\n\"\n",
    "    \n",
    "    if(save_results): # save all outputs to the save folder\n",
    "        \n",
    "        writer.add_scalars('Loss/Reconstruction', {'Train':train_rec_loss, 'Test':test_rec_loss}, i)\n",
    "        writer.add_scalars('Loss/Segmentation', {'Train':train_seg_loss, 'Test':test_seg_loss}, i)\n",
    "        writer.add_scalars('Loss/Accuracy', {'Train':train_acc, 'Test':test_acc}, i)\n",
    "\n",
    "        with open(log_folder + \"prints.txt\",\"a\") as file: \n",
    "            file.write(writeString)\n",
    "\n",
    "        if(i%50==0):\n",
    "            test_samples = next(iter(val_loader))\n",
    "            _, rec_loss, seg_loss, test_output = test_batch(test_samples)\n",
    "            \n",
    "            #ims = utils.plotPC([test_samples.numpy(), test_output], show=False)\n",
    "            #writer.add_figure(\"ims\", ims, i)\n",
    "            writer.flush()\n",
    "            \n",
    "        if(i%50==0):\n",
    "            utils.plotPC([test_samples[0:10].numpy(), segmentall(test_output[0:10])], show=False, save=True, name = (log_folder  + \"epoch_\" + str(i)))\n",
    "\n",
    "    else : # display all outputs\n",
    "        \n",
    "        test_samples = next(iter(val_loader))\n",
    "        loss , test_output = test_batch(test_samples)\n",
    "        utils.plotPC([test_samples,test_output])\n",
    "\n",
    "        print(writeString)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.cpu().state_dict(), log_folder + \"model_state_dict\")\n",
    "torch.save(model.cpu(), log_folder + \"model_save\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
