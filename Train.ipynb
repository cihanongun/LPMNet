{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import model\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "log_folder = \"logs/\" # folder path to save the results\n",
    "save_results = True # save the results to log_folder\n",
    "latent_size = 128 # bottleneck size of the Autoencoder model\n",
    "\n",
    "category = \"Chair\"\n",
    "n_points = 2048\n",
    "\n",
    "if(save_results):\n",
    "    utils.clear_folder(log_folder)\n",
    "    writer = SummaryWriter(log_folder + \"TB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.load_dataset import get_dataset\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_set = get_dataset(category, \"train\", n_points)\n",
    "val_set = get_dataset(category, \"validation\", n_points)\n",
    "\n",
    "part_count = int(train_set.max())\n",
    "\n",
    "print(\"Train set shape :\" + str(train_set.shape))\n",
    "print(\"Validation set shape :\" + str(val_set.shape))\n",
    "print(\"Number of points : \" + str(n_points))\n",
    "print(\"Part count : \" + str(part_count))\n",
    "\n",
    "train_tensor = torch.from_numpy(train_set).float()\n",
    "val_tensor = torch.from_numpy(val_set).float()\n",
    "\n",
    "train_loader = DataLoader(dataset=train_tensor, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(dataset=val_tensor, batch_size=batch_size, shuffle=True,  pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.LPMNet(n_points, latent_size, part_count)\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.loss import chamfer_distance # chamfer distance for calculating point cloud distance\n",
    "\n",
    "def rec_criterion(pc1, pc2):\n",
    "    loss, _ = chamfer_distance(pc1, pc2)\n",
    "    return loss\n",
    "\n",
    "seg_criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch_n):\n",
    "    model.train()\n",
    "    t_rec_loss, t_seg_loss , t_accuracy = 0,0,0\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        labels = data[:,:,3].to(device).long()\n",
    "        \n",
    "        points = data[:,:,0:3].to(device)\n",
    "        \n",
    "        seg_results, output = model(points)\n",
    "        rec_loss = rec_criterion(points, output)\n",
    "        \n",
    "        seg_loss = seg_criterion( seg_results.view(-1,part_count+1) ,labels.view(-1))\n",
    "\n",
    "        seg_labels = seg_results.argmax(dim=2,keepdim=True).squeeze()\n",
    "        correct = seg_labels.eq(labels.data).cpu().sum()\n",
    "        accuracy = correct.item()/float(data.shape[0]*data.shape[1])\n",
    "        \n",
    "        loss = rec_loss + seg_loss\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        t_rec_loss += rec_loss.item()\n",
    "        t_seg_loss += seg_loss.item()\n",
    "        t_accuracy += accuracy\n",
    "        \n",
    "    model.eval()\n",
    "    return t_rec_loss/(i+1) , t_seg_loss/(i+1), t_accuracy/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(epoch_n):\n",
    "    \n",
    "    t_rec_loss, t_seg_loss , t_accuracy = 0,0,0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, data in enumerate(val_loader):\n",
    "\n",
    "            labels = data[:,:,3].to(device).long()\n",
    "\n",
    "            points = data[:,:,0:3].to(device)\n",
    "\n",
    "            seg_results, output = model(points)\n",
    "\n",
    "            rec_loss = rec_criterion(points, output)\n",
    "\n",
    "            seg_loss = seg_criterion( seg_results.view(-1,part_count+1) ,labels.view(-1))\n",
    "\n",
    "            seg_labels = seg_results.argmax(dim=2,keepdim=True).squeeze()\n",
    "            correct = seg_labels.eq(labels.data).cpu().sum()\n",
    "            accuracy = correct.item()/float(data.shape[0]*data.shape[1])\n",
    "\n",
    "            t_rec_loss += rec_loss.item()\n",
    "            t_seg_loss += seg_loss.item()\n",
    "            t_accuracy += accuracy\n",
    "        \n",
    "    return t_rec_loss/(i+1) , t_seg_loss/(i+1), t_accuracy/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(data): # test with a batch of inputs\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        labels = data[:,:,3].to(device).long()\n",
    "        points = data[:,:,0:3].to(device)\n",
    "        \n",
    "        seg_results, output = model(points)\n",
    "        rec_loss = rec_criterion(points, output)\n",
    "        \n",
    "        seg_loss = seg_criterion( seg_results.view(-1,part_count+1) ,labels.view(-1))\n",
    "\n",
    "        seg_labels = seg_results.argmax(dim=2,keepdim=True).squeeze()\n",
    "        correct = seg_labels.eq(labels.data).cpu().sum()\n",
    "        accuracy = correct.item()/float(data.shape[0]*data.shape[1])\n",
    "        \n",
    "        loss = seg_loss + rec_loss\n",
    "        \n",
    "    return accuracy, rec_loss.item(), seg_loss.item(), output.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentall(pc):\n",
    "    \n",
    "    t_data = torch.cat([pc, torch.zeros([pc.shape[0],n_points,1]).to(pc.device)],2)\n",
    "\n",
    "    seg_results, output = model(t_data.to(device))\n",
    "        \n",
    "    seg_labels = seg_results.argmax(dim=2,keepdim=True).squeeze()\n",
    "        \n",
    "    t_data[:,:,3] = seg_labels\n",
    "    \n",
    "    return t_data.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(101) :\n",
    "\n",
    "    startTime = time.time()\n",
    "    \n",
    "    train_rec_loss, train_seg_loss, train_acc = train_epoch(i)\n",
    " \n",
    "    test_rec_loss, test_seg_loss, test_acc = test_epoch(i) # test with test set\n",
    "    \n",
    "    epoch_time = time.time() - startTime\n",
    "    \n",
    "    writeString = \"epoch \" + str(i) + \" epoch time : \" + str(epoch_time) + \"\\n\" + \\\n",
    "                  \"Train --> Rec : \" + str(train_rec_loss) + \" Seg : \" + str(train_seg_loss) + \" Acc : \" + str(train_acc) + \"\\n\" + \\\n",
    "                  \"Validation --> Rec : \" + str(test_rec_loss) + \" Seg : \" + str(test_seg_loss) + \" Acc : \" + str(test_acc) + \"\\n\"\n",
    "    \n",
    "    if(save_results): # save all outputs to the save folder\n",
    "        \n",
    "        writer.add_scalars('Loss/Reconstruction', {'Train':train_rec_loss, 'Test':test_rec_loss}, i)\n",
    "        writer.add_scalars('Loss/Segmentation', {'Train':train_seg_loss, 'Test':test_seg_loss}, i)\n",
    "        writer.add_scalars('Loss/Accuracy', {'Train':train_acc, 'Test':test_acc}, i)\n",
    "\n",
    "        with open(log_folder + \"prints.txt\",\"a\") as file: \n",
    "            file.write(writeString)\n",
    "\n",
    "        if(i%50==0):\n",
    "            test_samples = next(iter(val_loader))\n",
    "            _, rec_loss, seg_loss, test_output = test_batch(test_samples)\n",
    "            \n",
    "            #ims = utils.plotPC([test_samples.numpy(), test_output], show=False)\n",
    "            #writer.add_figure(\"ims\", ims, i)\n",
    "            writer.flush()\n",
    "            \n",
    "        if(i%50==0):\n",
    "            utils.plotPC([test_samples[0:10].numpy(), segmentall(test_output[0:10])], show=False, save=True, name = (log_folder  + \"epoch_\" + str(i)))\n",
    "\n",
    "    else : # display all outputs\n",
    "        \n",
    "        test_samples = next(iter(val_loader))\n",
    "        loss , test_output = test_batch(test_samples)\n",
    "        utils.plotPC([test_samples,test_output])\n",
    "\n",
    "        print(writeString)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.cpu().state_dict(), log_folder + \"model_state_dict\")\n",
    "torch.save(model.cpu(), log_folder + \"model_save\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
